{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Experimentation\n",
    "Feel free to peek at the repo, but I'm simply using Jupyter Notebooks to explore the LangChain framework and gain undertanding of how it interacts with LLM's and supports things like RAG and Agentic Workflows. I have no specific goal in mind. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipenv install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the LLM\n",
    "You'll need a `.env` file with the following variables:\n",
    "```text\n",
    "HUGGINGFACE_API_TOKEN=api_key\n",
    "```\n",
    "You can alternatively downgrade the `repo_id` to the Llama3 8B Instruct model, which \n",
    "should be available for free. Or, swap out the LLM for a different one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_endpoint import HuggingFaceEndpoint\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import os\n",
    "\n",
    "HF_API_TOKEN = os.environ['HUGGINGFACE_API_TOKEN']\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=\"meta-llama/Meta-Llama-3-70B-Instruct\", \n",
    "                             temperature=0.001,\n",
    "                             streaming=True,\n",
    "                             callbacks=[StreamingStdOutCallbackHandler()],\n",
    "                             huggingfacehub_api_token=HF_API_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Content\n",
    "You'll see commented-out code that was used to load a file with the declaration of independence. However, this has been replaced by a YouTube video transcript loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import TextLoader\n",
    "# SOURCE = \"./declaration-of-independence.txt\"\n",
    "# loader = TextLoader(SOURCE)\n",
    "\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "SOURCE=\"https://youtu.be/2XlYSmIlpfs?si=qZSiBtDroP1vvvJX\"\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    SOURCE, add_video_info=False\n",
    ")\n",
    "docs = loader.load()\n",
    "char_count = len(docs[0].page_content)\n",
    "print(f\"Char count in {SOURCE}: {char_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Content into Chunks\n",
    "We want the content to be split into chunks that are small enough to be processed by the LLM. That is, the context window needs to accomodate all chunks passed to it as relevant information, the query, the template/instructions, and the respose. So, we split the content into chunks of 100 tokens with a 10 token overlap. \n",
    "\n",
    "We also add metadata for each chunk that can be used for later processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import  RecursiveCharacterTextSplitter\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer: GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "cts = RecursiveCharacterTextSplitter(separators=[\"|\\n\\n\", \"\\n\", \" \", \"\"])\n",
    "splitter = cts.from_huggingface_tokenizer(tokenizer=tokenizer, chunk_size=100, chunk_overlap=10)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "print(f\"Split document count: {len(split_docs)}\")\n",
    "\n",
    "for index, doc in enumerate(split_docs):\n",
    "    doc.metadata.update({\"chunk_id\": index})\n",
    "    doc.metadata.update({\"token_count\": len(tokenizer.tokenize(doc.page_content))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize (Create Embeddings)\n",
    "The content needs to be vectorized so that we can use Semantic Search to find relevant chunks. The chosen sentence transformer is part of the inference API offered by Hugging Face PRO; if you do not have an API key you can just delete the `model_name` parameter and use one that is freely available on Hugging Face. Alternatively, you can swap out the Hugging Face embeddings for the OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=HF_API_TOKEN, model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "doc_content = [doc.page_content for doc in split_docs]\n",
    "vectorized_docs = embeddings.embed_documents(doc_content)\n",
    "len(vectorized_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Query and Query Vector\n",
    "We generate a query and vectorize it. This query will be used to find relevant chunks in the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Make a detailed summary of the youtube video conversation.\"\n",
    "query_vector = embeddings.embed_query(query)\n",
    "len(query_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Facebook AI Semantic Search (FAISS) database\n",
    "We create a FAISS database with the embeddings of the content chunks. This will allow us to quickly find relevant chunks when we search for them using the query. The benefit of vectorized embeddings is that we can use cosine similarity to find the most similar chunks to the query. That is, we can perform a mathematical search which is much more powerful than a literal string search as we can encode meaning into the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(split_docs, embeddings)\n",
    "found = db.similarity_search_by_vector(query_vector, k=30)\n",
    "sorted_found = sorted(found, key=lambda x: x.metadata[\"chunk_id\"])\n",
    "sorted_found[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the LLM\n",
    "The LLM is executed with the query and the relevant chunks. The LLM will generate a response for each chunk. We can then use the response to generate a summary of the content. The template used to generate the response is a simple one that asks the LLM to generate a response to the query and you can easily modify it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "template=\"\"\"\n",
    "You are a cryptocurrency policy export. You will answer\n",
    "the question below using the relevant information from the\n",
    "similar chunks of documents provided.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Relevant Information: {similar_chunks}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"similar_chunks\"])\n",
    "\n",
    "chain: Runnable = prompt | llm\n",
    "\n",
    "chain.invoke({\"question\": query, \"similar_chunks\": [doc.page_content for doc in sorted_found]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-learning-KI_9i9mN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
